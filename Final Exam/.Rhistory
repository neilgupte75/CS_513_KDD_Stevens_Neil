?factor()
cat<-c("good,"bad,"good","bad","good","bad")
cat<-c("good,"bad","good","bad","good","bad")
cat<-c("good","bad","good","bad","good","bad")
typeof(cat)
cat2<-factor(cat)
cat2
typeof(cat)
typeof(cat2)
cat3<-factor(cat,levels=(c("good","bad")))
cat3
catnum<-as.numeric(cat3)
catnum
days_fac<-factor(days)
days<-c("mon","tues","wed","thurs","fri")
days_fac<-factor(days)
days_fac
my.lst<-list(34445,c("neil","asdads"),c(2,1,2))
my.lst
my.lst<-list(stud.id=34445,stud.name=c("neil","asdads"),stud.marks=c(2,1,2))
my.lst
is.list(my.lst)
mode(my.lst)
typeof(my.lst)
length(my.lst)
my.lst2<-list(seq=1:10,my.lst)
my.lst2
my.lst2
length(my.lst)
length(my.lst2)
my.dataset<-data.frame(site=c('A','B','C','D','E'),ph=c(1,2,3,4,5))
length(my.dataset)
is.list(my.dataset)
is.matrix(my.dataset)
is.data.frame(my.dataset)
typeof(my.dataset)
View(my.dataset)
data("iris")
data(iris)
view(iris)
View(iris)
length(iris)
nrow(iris)
is.data.frame(iris)
read.csv()
?read.csv()
?factor
?read.csv()
avector<-c(1,2,3,4)
typeof(avector)
second<-avector[2]
second
second<-avector["second"]
second
avector
names(avector)<-c("first","second","third","fourth")
second<-avector["second"]
second
second<-avector[2]
second
avector
avector[22]
avector[2]<-22
avector
my.lst<-list(stud.id=34445,stud.name=c("neil","asdads"),stud.marks=c(2,1,2))
my.lst[2]
typeof(my.lst[2])
my.lst2[[2]]
my.lst[[2]]
typeof(my.lst[[2]])
is.vector(my.lst[[2]])
element2<-my.lst[[2]]
ln<-my.lst[[2]][2]
ln
rm(ls)
rm(list=ls())
data()
data(iris)
View(iris)
iris[5,4]
iris[-3,]
iris[,-3]
iris[-3,]
iris[,-3]#everything except 3 column
iris[-3,] #everything except 3 row
iris[c(13,5,10),c(5,2,4)]
subset4<-iris[c(T,F,F,F),]
subset4
?sample
sample(100,60)
sample(160,60)
idx<-sample(nrow(iris),as.integer(.65*nrow(iris))
idx
idx<-sample(nrow(iris),as.integer(.65*nrow(iris)))
idx
training<-iris[idx,]
test<-iris[-idx,]
idx<-seq(1,nrow(iris),5)
idx
vec<-1:20
mnnorm(vec)
mnnorm<-function(x.minx,maxx)
{
z<-((x-minx)/(maxx-minx))
return(z)
}
mnnorm(vec)
mnnorm<-function(x,minx,maxx)
{
z<-((x-minx)/(maxx-minx))
return(z)
}
vec<-1:20
mnnorm(vec)
mnnorm(vec,1,20)
install.packages(c("kknn", "VIM"))
rm(list=ls())
data(iris)
View(iris)
length(iris)
nrow(iris)
iris_missing<-iris
iris_missing[c(3,30,40),3]<-NA
View(iris_missing)
View(iris_missing)
View(iris)
View(iris)
summary(iris_missing)
?boxplot()
boxplot(iris[1:3])  ### sepal width contains some outliers ###
?hist()
hist(iris$Sepal.Length)
?pairs()
pairs(iris[1:2])
pairs(iris[1:4])
pairs(iris[1:4], main="Iris Data", pch=10)
pairs(iris[1:4])
pairs(iris[1:4], main="Iris Data", pch=1)
pairs(iris[1:4], main="Iris Data", pch=20)
pairs(iris[1:4], main="Iris Data", pch=10)
pairs(iris[1:4], main="Anderson's Iris Data -- 3 species", pch=21, bg=c("red", "green3","blue")[factor(iris$species)])
pairs(iris[1:4], main="Anderson's Iris Data -- 3 species", pch=21, bg=c("red", "green3","blue")[factor(iris$Species)])
?plot()
?plot()
plot(iris[,1:2])
?na.omit
iris_missing<-na.omit(iris_missing)
x<-c(1,2,6,6,6,7,7,7)
unique.x<-unique(x)
?match()
match(x, unique.x) ### matches the elements in x with unique.x and gives the index of the matched position ###
?tabulate()
tab<-tabulate(match(x, unique.x)) ### tabulate: counts the repitition ###
unique.x[tab==max(tab)]
max(x)
max(unique.x)
max(tab)
mfv <- function(x) {
unique.x<-unique(x)
tab<-tabulate(match(x, unique.x))
unique.x[tab==max(tab)]
}
mfv.x<-mfv(x)
mfv.x<-mfv(x)
mfv.x               ### the most frequent value may not be unique ###
is.vector(mfv.x)
?sample()
library("kknn")
?kknn()
predict_k5<-kknn(formula=Species~.,training, test[,-5], k=5, kernel='rectangular')
index <- seq(1,nrow(iris),by=5)
index
test<-iris[index,]
training<-iris[-index,]
predict_k5<-kknn(formula=Species~.,training, test[,-5], k=5, kernel='rectangular')
predict_k5
fit<-fitted(predict_k5)
table(Actual=test$Species, fit)
iris_normalized<-as.data.frame (
cbind( Sepal.Length=mmnorm(iris[,1],min(iris[,1]),max(iris[,1]))
, sepal.Width=mmnorm(iris[,2],min(iris[,2]),max(iris[,2] ))
,Petal.Length=mmnorm(iris[,3],min(iris[,3]),max(iris[,3] ))
, Petal.Width=mmnorm(iris[,4],min(iris[,4]),max(iris[,4] ))
,Species=as.character(iris[,5])
)
)
###  rectangular: unweighted, default: Euclidian, k=5: picking the 5 nearest neighbours  ###
predict_k5<-kknn(formula=Species~.,training, test[,-5], k=5, kernel='rectangular')
predict_k5
predict_k5
table(Actual=test$Species, fit)
test<-iris_normalized[index,]
training <-iris_normalized[-index,]
predict_k5 <- kknn(formula=Species~., training, test[,-5], k=5,kernel ="triangular" )
fit <- fitted(predict_k5)
table(Actual=test$Species,fit)
install.packages("VIM")
library('VIM')
rm(list=ls())
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
install.packages("rpart.plot")     # Enhanced tree plots
install.packages("rpart.plot")
install.packages("rpart")
install.packages("rpart.plot")     # Enhanced tree plots
install.packages("rattle")         # Fancy tree plot
install.packages("RColorBrewer")   # colors needed for rattle
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
filename<-file.choose()
dsn<-  read.csv(filename )
set.seed(111)
?ifelse
index<-sort(sample(nrow(dsn),round(.25*nrow(dsn))))
training<-dsn[-index,]
test<-dsn[index,]
dev.off()
CART_class<-rpart( Survived~.,data=training)
rpart.plot(CART_class)
CART_predict2<-predict(CART_class,test, type="class")
table(Actual=test[,4],CART=CART_predict2)
CART_predict<-predict(CART_class,test)
CART_predict<-predict(CART_class,test)
str(CART_predict)
CART_predict_cat<-ifelse(CART_predict[,1]<=.5,'Yes','No')
table(Actual=test[,4],CART=CART_predict_cat)
CART_wrong<-sum(test[,4]!=CART_predict_cat)
CART_error_rate<-CART_wrong/length(test[,4])
CART_error_rate
CART_predict2<-predict(CART_class,test, type="class")
CART_wrong2<-sum(test[,4]!=CART_predict2)
CART_error_rate2<-CART_wrong2/length(test[,4])
CART_error_rate2
install.packages("C50")
rm(list=ls())
filename<-file.choose()
dsn<-  read.csv(filename )
dev.off
?na.omit()
dsn2<-na.omit(dsn)
set.seed(123)
?ifelse
index<-sort(sample(nrow(dsn),round(.25*nrow(dsn))))
training<-dsn[-index,]
test<-dsn[index,]
#install.packages("C50", repos="http://R-Forge.R-project.org")
#install.packages("C50")
library('C50')
C50_class <- C5.0( Survived~.,data=training )
summary(C50_class )
dev.off()
plot(C50_class)
C50_predict<-predict( C50_class ,test , type="class" )
table(actual=test[,4],C50=C50_predict)
wrong<- (test[,4]!=C50_predict)
c50_rate<-sum(wrong)/length(test[,4])
c50_rate
plot(C50_class)
install.packages("randomForest")
library(randomForest)
setwd("C:/cs 513 HOMEWORK/Final Exam")
rm(list=ls())
df <- read.csv('Admission.csv',na.strings = "?")
head(df, n=5)
summary(df)
nrow(df)
df <- na.omit(df)
nrow(df)
?kmeans
df
View(df)
kmeans_1<- kmeans(df[,c(-1,-2,-5)],2,nstart = 10)
kmeans_1$cluster
conf_matrix = table(kmeans_1$cluster,df[,2])
print(conf_matrix)
hclust_dist<-dist(df[,c(-1,-2,-5)])
hclust_results<-hclust(hclust_dist)
plot(hclust_results)
hclust_2<-cutree(hclust_results,2)
table(hclust_2,df[,2])
rm(list=ls())
df <- read.csv('Admission.csv',na.strings = "?")
head(df, n=5)
#Summarizing each column (e.g. min, max, mean )
summary(df)
#Check the number of rows before
nrow(df)
#Remove the rows with missing values
df <- na.omit(df)
#Check the number of rows after
nrow(df)
?kmeans
View(df)
# kmeans
kmeans_1<- kmeans(df[,c(-1,-2,-5)],2,nstart = 10)
kmeans_1$cluster
conf_matrix = table(kmeans_1$cluster,df[,2])
print(conf_matrix)
#hclust
hclust_dist<-dist(df[,c(-1,-2,-5)])
hclust_results<-hclust(hclust_dist)
plot(hclust_results)
hclust_2<-cutree(hclust_results,2)
conf_matrix2=table(hclust_2,df[,2])
print(conf_matrix2)
df <- read.csv('Admission_cat.csv',na.strings = "?")
rm(list=ls())
df <- read.csv('Admission_cat.csv',na.strings = "?")
size <- floor(0.70 * nrow(df))
set.seed(123)
random <- sample(seq_len(nrow(df)), size = size)
train <- df[random, ]
test <- df[-random, ]
View(df)
rm(list=ls())
df <- read.csv('Admission_cat.csv',na.strings = "?")
View(df)
size <- floor(0.70 * nrow(df))
summary(df)
size <- floor(0.70 * nrow(df))
#Set the seed
set.seed(123)
random <- sample(seq_len(nrow(df)), size = size)
train <- df[random, ]
test <- df[-random, ]
fit <- randomForest(ADMIT~., data=train[,c(-1,-2)],importance=TRUE, ntree=100)
rm(list=ls())
library(randomForest)
df <- read.csv('Admission_cat.csv',na.strings = "?")
View(df)
summary(df)
size <- floor(0.70 * nrow(df))
#Set the seed
set.seed(123)
random <- sample(seq_len(nrow(df)), size = size)
#70% data in train dataset
train <- df[random, ]
#30% data in test dataset
test <- df[-random, ]
fit <- randomForest(ADMIT~., data=train[,c(-1,-2)],importance=TRUE, ntree=100)
fit <- randomForest(ADMIT~., data=train[,c(-1,)],importance=TRUE, ntree=100)
fit <- randomForest(ADMIT~., data=train[,c(-1)],importance=TRUE, ntree=100)
fit <- randomForest(ADMIT~., data=train[,c(-1)],importance=TRUE, ntree=100)
importance(fit)
varImpPlot(fit)
pred <- predict(fit, test)
print(pred)
conf_matrix <- table(pred,class=test$ADMIT)
print(conf_matrix)
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(conf_matrix)
wrong<- (test$ADMIT!=pred)
error_rate<-sum(wrong)/length(wrong)
error_rate
wrong<- (test$ADMIT!=pred)
#Error rate
error_rate<-sum(wrong)/length(wrong)
error_rate
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(conf_matrix)
rm(list=ls())
library(C50)
df <- read.csv('Admission_cat.csv',na.strings = "?")
View(df)
summary(df)
size <- floor(0.70 * nrow(df))
set.seed(123)
random <- sample(seq_len(nrow(df)), size = size)
train <- df[random, ]
test <- df[-random, ]
model<-C5.0(ADMIT~.,train[,-1])
summary(model)
model<-C5.0(ADMIT~.,train[,-1])
df$ADMIT <- factor(df$ADMIT, levels = c(0,1))
rm(list=ls())
library(C50)
df <- read.csv('Admission_cat.csv',na.strings = "?")
View(df)
summary(df)
df$ADMIT <- factor(df$ADMIT, levels = c(0,1))
size <- floor(0.70 * nrow(df))
#Set the seed
set.seed(123)
random <- sample(seq_len(nrow(df)), size = size)
train <- df[random, ]
#30% data in test dataset
test <- df[-random, ]
model<-C5.0(ADMIT~.,train[,-1])
summary(model)
plot(model)
prediction<-predict(model,test[,-1],type="class")
conf_matrix<-table(test[,2],prediction)
conf_matrix
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(conf_matrix)
rm(list=ls())
library(C50)
df <- read.csv('Admission_cat.csv',na.strings = "?")
View(df)
summary(df)
df$ADMIT <- factor(df$ADMIT, levels = c(0,1))
size <- floor(0.70 * nrow(df))
#Set the seed
set.seed(123)
random <- sample(seq_len(nrow(df)), size = size)
#70% data in train dataset
train <- df[random, ]
#30% data in test dataset
test <- df[-random, ]
#c5.0 model
model<-C5.0(ADMIT~.,train[,-1])
summary(model)
plot(model)
prediction<-predict(model,test[,-1],type="class")
#confusion matrix
conf_matrix<-table(test[,2],prediction)
conf_matrix
# accuracy of model
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(conf_matrix)
#  Course          :CS 513 Data Mining
#  First Name      : Neil
#  Last Name       : Gupte
#  Id              : 10445674
#  purpose         : q1 final exam
rm(list=ls())
df <- read.csv('Admission.csv',na.strings = "?")
head(df, n=5)
#Summarizing each column (e.g. min, max, mean )
summary(df)
#Check the number of rows before
nrow(df)
#Remove the rows with missing values
df <- na.omit(df)
#Check the number of rows after
nrow(df)
?kmeans
View(df)
# kmeans
kmeans_1<- kmeans(df[,c(-1,-2,-5)],2,nstart = 10)
kmeans_1$cluster
conf_matrix = table(kmeans_1$cluster,df[,2])
print(conf_matrix)
#hclust
hclust_dist<-dist(df[,c(-1,-2,-5)])
hclust_results<-hclust(hclust_dist)
plot(hclust_results)
hclust_2<-cutree(hclust_results,2)
conf_matrix2=table(hclust_2,df[,2])
print(conf_matrix2)
# CS 513 HW
# Name: Neil Gupte
# CWID:10445674
# purpose: final exam q2
rm(list=ls())
library(randomForest)
df <- read.csv('Admission_cat.csv',na.strings = "?")
View(df)
summary(df)
size <- floor(0.70 * nrow(df))
#Set the seed
set.seed(123)
random <- sample(seq_len(nrow(df)), size = size)
#70% data in train dataset
train <- df[random, ]
#30% data in test dataset
test <- df[-random, ]
fit <- randomForest(ADMIT~., data=train[,c(-1)],importance=TRUE, ntree=100)
importance(fit)
varImpPlot(fit)
pred <- predict(fit, test)
print(pred)
#Confusion Matrix
conf_matrix <- table(pred,class=test$ADMIT)
print(conf_matrix)
#Accuracy
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(conf_matrix)
#  Course          :CS 513 Data Mining
#  First Name      : Neil
#  Last Name       : Gupte
#  Id              : 10445674
#  purpose         : q3 final exam
rm(list=ls())
library(C50)
df <- read.csv('Admission_cat.csv',na.strings = "?")
View(df)
summary(df)
df$ADMIT <- factor(df$ADMIT, levels = c(0,1))
size <- floor(0.70 * nrow(df))
#Set the seed
set.seed(123)
random <- sample(seq_len(nrow(df)), size = size)
#70% data in train dataset
train <- df[random, ]
#30% data in test dataset
test <- df[-random, ]
#c5.0 model
model<-C5.0(ADMIT~.,train[,-1])
summary(model)
plot(model)
prediction<-predict(model,test[,-1],type="class")
#confusion matrix
conf_matrix<-table(test[,2],prediction)
conf_matrix
# accuracy of model
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(conf_matrix)
